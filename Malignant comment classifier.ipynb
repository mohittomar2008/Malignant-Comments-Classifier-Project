{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "80fd25c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk import word_tokenize,sent_tokenize,TweetTokenizer,RegexpTokenizer\n",
    "\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "345e0cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('https://raw.githubusercontent.com/mohittomar2008/Malignant-Comments-Classifier-Project/main/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8770d05d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>malignant</th>\n",
       "      <th>highly_malignant</th>\n",
       "      <th>rude</th>\n",
       "      <th>threat</th>\n",
       "      <th>abuse</th>\n",
       "      <th>loathe</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000997932d777bf</td>\n",
       "      <td>Explanation\\nWhy the edits made under my usern...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>000103f0d9cfb60f</td>\n",
       "      <td>D'aww! He matches this background colour I'm s...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000113f07ec002fd</td>\n",
       "      <td>Hey man, I'm really not trying to edit war. It...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0001b41b1c6bb37e</td>\n",
       "      <td>\"\\nMore\\nI can't make any real suggestions on ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0001d958c54c6e35</td>\n",
       "      <td>You, sir, are my hero. Any chance you remember...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  \\\n",
       "0  0000997932d777bf  Explanation\\nWhy the edits made under my usern...   \n",
       "1  000103f0d9cfb60f  D'aww! He matches this background colour I'm s...   \n",
       "2  000113f07ec002fd  Hey man, I'm really not trying to edit war. It...   \n",
       "3  0001b41b1c6bb37e  \"\\nMore\\nI can't make any real suggestions on ...   \n",
       "4  0001d958c54c6e35  You, sir, are my hero. Any chance you remember...   \n",
       "\n",
       "   malignant  highly_malignant  rude  threat  abuse  loathe  \n",
       "0          0                 0     0       0      0       0  \n",
       "1          0                 0     0       0      0       0  \n",
       "2          0                 0     0       0      0       0  \n",
       "3          0                 0     0       0      0       0  \n",
       "4          0                 0     0       0      0       0  "
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b649361b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted? They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC. And please don't remove the template from the talk page since I'm retired now.89.205.38.27\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "00c7499e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Explanation\\nWhy the edits made under my username Hardcore Metallica Fan were reverted?',\n",
       " \"They weren't vandalisms, just closure on some GAs after I voted at New York Dolls FAC.\",\n",
       " \"And please don't remove the template from the talk page since I'm retired now.89.205.38.27\"]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we would be working on Comment_text column\n",
    "\n",
    "# First split corpus into sentences\n",
    "\n",
    "sentences=sent_tokenize(df.iloc[0,1])\n",
    "sentences\n",
    "#len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a12149f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Explanation', 'Why', 'the', 'edits', 'made', 'under', 'my', 'username', 'Hardcore', 'Metallica', 'Fan', 'were', 'reverted', '?'], ['They', 'were', \"n't\", 'vandalisms', ',', 'just', 'closure', 'on', 'some', 'GAs', 'after', 'I', 'voted', 'at', 'New', 'York', 'Dolls', 'FAC', '.'], ['And', 'please', 'do', \"n't\", 'remove', 'the', 'template', 'from', 'the', 'talk', 'page', 'since', 'I', \"'m\", 'retired', 'now.89.205.38.27']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentences into words\n",
    "\n",
    "words=[word_tokenize(sentence) for sentence in sentences]\n",
    "print(words)\n",
    "len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "013b6fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean special character\n",
    "\n",
    "import re\n",
    "corpus=[]\n",
    "for i in range(len(sentences)):\n",
    "    review=re.sub('[^a-zA-Z]',' ',sentences[i])\n",
    "    review=review.lower()\n",
    "    corpus.append(review)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "ebfe25a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['explanation why the edits made under my username hardcore metallica fan were reverted ',\n",
       " 'they weren t vandalisms  just closure on some gas after i voted at new york dolls fac ',\n",
       " 'and please don t remove the template from the talk page since i m retired now             ']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "8415701a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# install. load stopwords\n",
    "\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dc1dba1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stemming   but not on stopwords\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "be8e6b96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "837f0ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  install libraries for stemmer and lemitizer\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer  # These are classes so create object for these classes before using them\n",
    "\n",
    "stemmer= PorterStemmer()\n",
    "lemmitize= WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "74324509",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['explanation why the edits made under my username hardcore metallica fan were reverted ',\n",
       " 'they weren t vandalisms  just closure on some gas after i voted at new york dolls fac ',\n",
       " 'and please don t remove the template from the talk page since i m retired now             ']"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus   # Corpus is paragraph into sentences without any special character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "45019c98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explan\n",
      "edit\n",
      "made\n",
      "usernam\n",
      "hardcor\n",
      "metallica\n",
      "fan\n",
      "revert\n",
      "vandal\n",
      "closur\n",
      "ga\n",
      "vote\n",
      "new\n",
      "york\n",
      "doll\n",
      "fac\n",
      "pleas\n",
      "remov\n",
      "templat\n",
      "talk\n",
      "page\n",
      "sinc\n",
      "retir\n"
     ]
    }
   ],
   "source": [
    "# Now, we will stemming mean find root word which is not in stopword dictionary\n",
    "\n",
    "for i in corpus:\n",
    "    words=word_tokenize(i)\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            print(stemmer.stem(word))\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "1ed255ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "explanation\n",
      "edits\n",
      "made\n",
      "username\n",
      "hardcore\n",
      "metallica\n",
      "fan\n",
      "reverted\n",
      "vandalism\n",
      "closure\n",
      "gas\n",
      "voted\n",
      "new\n",
      "york\n",
      "doll\n",
      "fac\n",
      "please\n",
      "remove\n",
      "template\n",
      "talk\n",
      "page\n",
      "since\n",
      "retired\n"
     ]
    }
   ],
   "source": [
    "# Now, we will lemmitize the word means find the root word with specific meaning. it seraches it in its own dictionary\n",
    "\n",
    "for i in corpus:\n",
    "    words=word_tokenize(i)\n",
    "    for word in words:\n",
    "        if word not in set(stopwords.words('english')):\n",
    "            print(lemmitize.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "fa406c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stopwords and lemmitize word\n",
    "final=[]\n",
    "for i in range(len(sentences)):\n",
    "    review=re.sub('[^a-zA-z]',' ',sentences[i])\n",
    "    review=review.lower()\n",
    "    review=review.split()\n",
    "    review=[lemmitize.lemmatize(word) for word in review if word not in set(stopwords.words('english'))]\n",
    "    review=' '.join(review)\n",
    "    final.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "99eeb393",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['explanation edits made username hardcore metallica fan reverted',\n",
       " 'vandalism closure gas voted new york doll fac',\n",
       " 'please remove template talk page since retired']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec98cd3",
   "metadata": {},
   "source": [
    "## Bag of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "dc762d70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bag of Words    # importance of words/ frequency\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "c3d8f5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X=cv.fit_transform(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "eac34c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'explanation': 3, 'edits': 2, 'made': 8, 'username': 19, 'hardcore': 7, 'metallica': 9, 'fan': 5, 'reverted': 15, 'vandalism': 20, 'closure': 0, 'gas': 6, 'voted': 21, 'new': 10, 'york': 22, 'doll': 1, 'fac': 4, 'please': 12, 'remove': 13, 'template': 18, 'talk': 17, 'page': 11, 'since': 16, 'retired': 14}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "e585c417",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'explanation edits made username hardcore metallica fan reverted'"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "81ea4102",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0]], dtype=int64)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56611447",
   "metadata": {},
   "source": [
    "### Bag of words with binary, means if a word is repeated 2 or more times in a sentence, it will not count it as 2 or more, it will replace it with 1 only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "4929add3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(binary=True)\n",
    "\n",
    "x=cv.fit_transform(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "a8f65f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'explanation': 3, 'edits': 2, 'made': 8, 'username': 19, 'hardcore': 7, 'metallica': 9, 'fan': 5, 'reverted': 15, 'vandalism': 20, 'closure': 0, 'gas': 6, 'voted': 21, 'new': 10, 'york': 22, 'doll': 1, 'fac': 4, 'please': 12, 'remove': 13, 'template': 18, 'talk': 17, 'page': 11, 'since': 16, 'retired': 14}\n"
     ]
    }
   ],
   "source": [
    "print(cv.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "11b04518",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        0]], dtype=int64)"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f98c58f",
   "metadata": {},
   "source": [
    "### Bag of Wors with ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5ebcd27",
   "metadata": {},
   "source": [
    "N-grams is used to capture semantic meaning of the sentence, it can be bigrams, trigrams and more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "e323cc7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'explanation edits made': 2,\n",
       " 'edits made username': 1,\n",
       " 'made username hardcore': 5,\n",
       " 'username hardcore metallica': 13,\n",
       " 'hardcore metallica fan': 4,\n",
       " 'metallica fan reverted': 6,\n",
       " 'vandalism closure gas': 14,\n",
       " 'closure gas voted': 0,\n",
       " 'gas voted new': 3,\n",
       " 'voted new york': 15,\n",
       " 'new york doll': 7,\n",
       " 'york doll fac': 16,\n",
       " 'please remove template': 9,\n",
       " 'remove template talk': 10,\n",
       " 'template talk page': 12,\n",
       " 'talk page since': 11,\n",
       " 'page since retired': 8}"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(binary=True, ngram_range=(3,3))  # it made cobinations of 3 words\n",
    "\n",
    "cv.fit_transform(final)\n",
    "cv.vocabulary_  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "20f1471d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'explanation edits': 5, 'edits made': 3, 'made username': 12, 'username hardcore': 29, 'hardcore metallica': 10, 'metallica fan': 14, 'fan reverted': 7, 'explanation edits made': 6, 'edits made username': 4, 'made username hardcore': 13, 'username hardcore metallica': 30, 'hardcore metallica fan': 11, 'metallica fan reverted': 15, 'vandalism closure': 31, 'closure gas': 0, 'gas voted': 8, 'voted new': 33, 'new york': 16, 'york doll': 35, 'doll fac': 2, 'vandalism closure gas': 32, 'closure gas voted': 1, 'gas voted new': 9, 'voted new york': 34, 'new york doll': 17, 'york doll fac': 36, 'please remove': 20, 'remove template': 22, 'template talk': 27, 'talk page': 25, 'page since': 18, 'since retired': 24, 'please remove template': 21, 'remove template talk': 23, 'template talk page': 28, 'talk page since': 26, 'page since retired': 19}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv=CountVectorizer(binary=True, ngram_range=(2,3))  # it made cobinations of all 2 and 3 words\n",
    "\n",
    "cv.fit_transform(final)\n",
    "print(cv.vocabulary_ )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65c9d25c",
   "metadata": {},
   "source": [
    "###  Issue: In Bag of Words all words are captured in 1 or 0, it doesnot save the importance of words\n",
    "    \n",
    "    in term of matrix, if distance between 2 words are less, they will be look like similar however they can be totally different.\n",
    "    \n",
    "    so to capture the importance of words we use TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dd95c3",
   "metadata": {},
   "source": [
    "### TF-IDF =  Term Frequency - Inverse Document Frequency\n",
    "\n",
    "To capture the importance of words, it assign more weightage to rare words while creating vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "4ba8e257",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf=TfidfVectorizer(ngram_range=(1,1))\n",
    "x=tfidf.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "24101f17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'explanation': 7, 'why': 37, 'the': 29, 'edits': 6, 'made': 14, 'under': 31, 'my': 16, 'username': 32, 'hardcore': 12, 'metallica': 15, 'fan': 9, 'were': 35, 'reverted': 24, 'they': 30, 'weren': 36, 'vandalisms': 33, 'just': 13, 'closure': 3, 'on': 19, 'some': 26, 'gas': 11, 'after': 0, 'voted': 34, 'at': 2, 'new': 17, 'york': 38, 'dolls': 4, 'fac': 8, 'and': 1, 'please': 21, 'don': 5, 'remove': 22, 'template': 28, 'from': 10, 'talk': 27, 'page': 20, 'since': 25, 'retired': 23, 'now': 18}\n"
     ]
    }
   ],
   "source": [
    "print(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "77f842f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'explanation why the edits made under my username hardcore metallica fan were reverted '"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "c82c58b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.        , 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.28195987, 0.28195987, 0.        , 0.28195987,\n",
       "        0.        , 0.        , 0.28195987, 0.        , 0.28195987,\n",
       "        0.28195987, 0.28195987, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.28195987,\n",
       "        0.        , 0.        , 0.        , 0.        , 0.21443775,\n",
       "        0.        , 0.28195987, 0.28195987, 0.        , 0.        ,\n",
       "        0.28195987, 0.        , 0.28195987, 0.        ]])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae2c23ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
